{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLLl1Agum8O"
      },
      "source": [
        "# Pinecone vector store: querying the index\n",
        "\n",
        "copyright 2025, Denis Rothman\n",
        "\n",
        "The goal of this notebook is to query the individual namespaces of a Pinecone index to retrieve either an instruction scenario or classicl RAG data. The retrieval will then be managed by a AI controller in the following chapters.\n",
        "\n",
        "The notebook contains the following sections:\n",
        "* **Setting up the environment** with a file dowloading script, OpenAI, and Pinecone.\n",
        "* **Initializing the Pinecone index**\n",
        "* **Defining query functions** to embedd the user input, run a similarity search on the namespace of a Pinecone index and return a result\n",
        "* **Retrieving an instruction scenario** Run the query functions to retrieve instructions that will be used in the following chapters.\n",
        "* **Retrieving data** Run the query functions to retrieve data that will be used in the following chapters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://raw.githubusercontent.com/uCertifyLLC/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"openai_api.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7713608-7670-4d2d-8501-651d569151fd",
        "id": "G_PuE5rjhWAK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'openai_api.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7i8j5vnpatH",
        "outputId": "6b22687c-f3b6-4388-a105-c402c898b830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae00faa6-e310-425d-dcdc-cbe381b31b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import openai_api\n",
        "from openai_api import make_openai_api_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ufJsT54EHe",
        "outputId": "e82a5f63-d9ea-49ba-c7b5-e555501cba74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "outputId": "cd939226-afc6-48af-a1d1-d2db8b0f5385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eIyjdd25LjH",
        "outputId": "96f08b0c-cf4f-4541-d8a0-b3786c81b1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zPS_mgsdXKj",
        "outputId": "03de5a37-5e4c-4067-f91e-b492dec9b128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "#  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7AYljM0gq1",
        "outputId": "91a68c6c-32d0-4eb9-fe42-8cc062d180e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'data01': {'vector_count': 9}, 'genaisys': {'vector_count': 3}},\n",
              " 'total_vector_count': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "le9T_1u7mNxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Querying the Pinecone index"
      ],
      "metadata": {
        "id": "jqpmayeiRNNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving an instruction scenario"
      ],
      "metadata": {
        "id": "wSPryKnKTPQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your namespace and query text\n",
        "namespace = \"genaisys\"  # Example namespace\n",
        "query_text = \"The customers like the idea of travelling and learning. Provide your sentiment.\"\n",
        "\n",
        "# Call the query function\n",
        "text, target_id = query_vector_store(query_text, namespace)\n",
        "\n",
        "# Display the final output\n",
        "print(\"Final output:\")\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Target ID: {target_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRFGZ4hSqYH",
        "outputId": "b1621ff2-c5ea-4597-88b7-74117e69168b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying vector store...\n",
            "Processed query results:\n",
            "ID: 2, Score: 0.221010014\n",
            "Final output:\n",
            "Text: 200,Sentiment analysis  Read the content return a sentiment analysis nalysis on this text and provide a score with the label named : Sentiment analysis score followed by a numerical value between 0 and 1  with no + or - sign and  add an explanation to justify the score.\n",
            "Target ID: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving data"
      ],
      "metadata": {
        "id": "3gscUetFTWX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your namespace and query text\n",
        "namespace = \"data01\"  # Example namespace\n",
        "query_text = \"What did the CTO say about the different types of memory?\"\n",
        "\n",
        "# Call the query function\n",
        "text, target_id = query_vector_store(query_text, namespace)\n",
        "\n",
        "# Display the final output\n",
        "print(\"Final output:\")\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Target ID: {target_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c9ddd8-0d28-4e74-c07f-5b9e188973e0",
        "id": "F_uSpqjgS3K1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying vector store...\n",
            "Processed query results:\n",
            "ID: 3, Score: 0.571151137\n",
            "Final output:\n",
            "Text: We defined memoryless, short-term, long-term memory, and cross-topic memory. For the hybrid travel marketing campaign, we will distinguish semantic memory (facts) from episodic memory (personal events in time, for example). The CTO said that we will need to use episodic memories of past customer trips to make the semantic aspects of our trips more engaging.\n",
            "Target ID: 3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vub82Rjxa17a",
        "jmmBBZ7oa17b",
        "9kNLfjTfAnFR",
        "O03SZzGGAreV",
        "5NL93eSL-mLi"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}