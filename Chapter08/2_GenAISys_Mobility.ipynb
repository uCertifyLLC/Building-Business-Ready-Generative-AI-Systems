{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPMfRLHyY2AGF/RfMyiiSIv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GenAISys Mobility\n",
        "Adding a Generic Synthetic Trajectory Simulation and Predictive system to the book's GenAISys\n",
        "\n",
        "üìå **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "This notebook integrates the *Trajectory simulation and Prediction* functionality built in 1_Trajectory_simulation_and_prediction.ipynb\n",
        "\n",
        "The progam is implemented at three levels:  \n",
        "**1.The IPython interface** under the name \"mobility\" at user level   \n",
        "**2.The Handler Selection Mechanism** to include the mobility function in the handler registry      \n",
        "**3.The AI Functions** to implement the trajectory simulation and prediction in the AI functions library.    \n",
        "\n",
        "**Note:** You can choose to deactive DeepSeek or activate with the local installation(GPU required) or API options(CPU only required). In this notebook, DeepSeek was deactivated and only requires a CPU.\n"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖSetting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing `click` and gTTS\n",
        "\n",
        "**If you want to use text-to-speech in this notebook with `gTTS`, set `use_gtts` to `True` in the following cell.**\n",
        "\n",
        "`click`, a command line library, is required for gTTS. This cell will set up the correct 'click' version required for gTTS and *display a restart message*. After restarting, simply click `Run All` to continue.\n",
        "\n",
        "*The restart is necessary for Colab to take the new version into account.*\n",
        "\n"
      ],
      "metadata": {
        "id": "rMuumu9clMkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_gtts = False #activates Google TTS in Google Colab if True and deactivates if False"
      ],
      "metadata": {
        "id": "2gNWRgE_b5wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Conditional 'click' setup.\n",
        "# This cell will only modify 'click' IF the 'click' version is not 8.1.8.\n",
        "# If changes are made, a clear message will appear in the output, instructing you to restart the session.\n",
        "# After restarting, Colab will reconnect, and you can simply \"Run All\" from the top.\n",
        "\n",
        "import importlib.metadata\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Import for displaying rich output (HTML)\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "#---Begin gTTS True or\n",
        "current_click_version = None\n",
        "try:\n",
        "    current_click_version = importlib.metadata.version('click')\n",
        "except importlib.metadata.PackageNotFoundError:\n",
        "    pass\n",
        "\n",
        "if not use_gtts:\n",
        "    required_click_version = current_click_version\n",
        "else:\n",
        "    required_click_version = '8.1.8'\n",
        "#---END gTTS True or False\n",
        "print(\"--- Checking 'click' package version... ---\")\n",
        "\n",
        "try:\n",
        "    current_click_version = importlib.metadata.version('click')\n",
        "    print(f\"Currently installed 'click' version: {current_click_version}\")\n",
        "except importlib.metadata.PackageNotFoundError:\n",
        "    print(\"'click' package not found.\")\n",
        "\n",
        "# Check if current click version is not the required one\n",
        "if current_click_version != required_click_version:\n",
        "    print(f\"\\n--- 'click' version is not {required_click_version}. Initiating setup... ---\")\n",
        "\n",
        "    print(\"Uninstalling any existing 'click' installation...\")\n",
        "    !pip uninstall -y click\n",
        "\n",
        "    print(f\"Installing 'click=={required_click_version}' for compatibility...\")\n",
        "    !pip install click=={required_click_version}\n",
        "\n",
        "    print(\"\\n--- 'click' setup complete. ---\")\n",
        "\n",
        "    # Display a prominent, styled HTML message in the cell output\n",
        "    html_message = \"\"\"\n",
        "    <div style=\"\n",
        "        background-color: #e8f5e9; /* Very light green, calming */\n",
        "        border: 2px solid #4caf50; /* Green border */\n",
        "        padding: 15px;\n",
        "        margin: 15px 0;\n",
        "        border-radius: 8px;\n",
        "        font-size: 1.1em; /* Slightly smaller font */\n",
        "        color: #000000; /* Black text */\n",
        "    \">\n",
        "        <h3>Runtime Setup Complete</h3>\n",
        "        <p>Package versions have been updated. For changes to take effect, you <strong>MUST restart the Colab runtime now.</strong></p>\n",
        "        <p>Go to the menu: <strong><code>Runtime</code> &gt; <code>Restart runtime</code></strong>.</p>\n",
        "        <p>After reconnecting, simply click <strong><code>Run All</code></strong> from the top to continue.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html_message))\n",
        "\n",
        "    # Stop the Python cell execution gracefully.\n",
        "    raise SystemExit(\"Please restart the Colab runtime to apply changes.\")\n",
        "\n",
        "else:\n",
        "    print(f\"--- 'click' is already at the correct version ({required_click_version}). No action needed. ---\")"
      ],
      "metadata": {
        "id": "gWQ0EcnUlMkP",
        "outputId": "2eef7830-7747-409d-c2bd-5044a799dcf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking 'click' package version... ---\n",
            "Currently installed 'click' version: 8.2.1\n",
            "--- 'click' is already at the correct version (8.2.1). No action needed. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ **DeepSeek Activation Guide**  \n",
        "\n",
        "#### ‚úÖ **Option 1: Activate DeepSeek (`deepseek=True`)**  \n",
        "üîπ **Resource Requirements**:  \n",
        "- **GPU**: ~20GB VRAM (estimate)  \n",
        "- **Disk Space**: 30-40GB  \n",
        "\n",
        "üîπ **Setup Options**:  \n",
        "- **On Google Colab**:  \n",
        "  - **Recommended**: Google Colab **Pro** (with upgraded disk space).  \n",
        "  - **Check**: Potential cost considerations.  \n",
        "- **On a Local Machine**:  \n",
        "  - **Recommended**: A **recent laptop** with a GPU.  \n",
        "  - **No additional cost** required.  \n",
        "\n",
        "---\n",
        "\n",
        "#### ‚ùå **Option 2: No DeepSeek Activation (`deepseek=False`)**  \n",
        "üîπ **Resource Requirements**:  \n",
        "- **No GPU** required (CPU is sufficient).  \n",
        "- **No additional disk space needed**.  \n",
        "- **No cost** for local execution.  \n",
        "\n",
        "üîπ **Limitations**:  \n",
        "- DeepSeek **won‚Äôt be installed** in this notebook.  \n",
        "- The notebook will default to using the **OpenAI framework** (which requires an OpenAI API token and incurs costs).  \n",
        "- Alternatively, you can use the **DeepSeek API** via `pip install openai`, but API calls will be charged based on DeepSeek‚Äôs pricing.  \n",
        "\n",
        "---\n",
        "\n",
        "üí° **Key Takeaway**  \n",
        "This setup gives you flexibility to explore different execution environments and choose the best fit for your project! üöÄ\n"
      ],
      "metadata": {
        "id": "HMKlLbunNZEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek activation deepseek=True to activate. 20 Go (estimate) GPU memory and 30-40 Go Disk Space\n",
        "deepseek=False"
      ],
      "metadata": {
        "id": "lyhGldEwNeiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zHjDOMWo9Blf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://raw.githubusercontent.com/uCertifyLLC/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8Lnpx8Ywvkqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activate cache in Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if deepseek==True:\n",
        "  # Define the cache directory in your Google Drive\n",
        "  cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "  # Set environment variables to direct Hugging Face to use this cache directory\n",
        "  os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "  #os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !pip install transformers"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking transformer version"
      ],
      "metadata": {
        "id": "yKXESfW5zzpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  import transformers\n",
        "  print(transformers.__version__)"
      ],
      "metadata": {
        "id": "-0uKNItkz_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if deepseek==True:\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Hugging Face token from Colab's Secrets Manager: uncomment only if requested\n",
        "#if deepseek==True:\n",
        "  #from google.colab import userdata\n",
        "  #userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "lG45BHPj1Thc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"reason.py\")\n",
        "download(\"commons\",\"machine_learning.py\")"
      ],
      "metadata": {
        "id": "G_PuE5rjhWAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "id": "a7i8j5vnpatH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import make_openai_api_call\n",
        "from reason import make_openai_reasoning_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing gtts\n",
        "\n",
        "gTTS (Google Text-to-Speech) is a Python library and CLI tool that interfaces with Google Translate's text-to-speech API. It allows users to convert text into spoken words, supporting multiple languages and accents, and can save the output as MP3 files.  "
      ],
      "metadata": {
        "id": "pGpMI-5iLFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use_gtts activates Google TTS in Google Colab if use_gtts is True and deactivates if False\n",
        "if use_gtts:\n",
        " !pip install gTTS==2.5.4\n",
        " from gtts import gTTS\n",
        " from IPython.display import Audio\n",
        "\n",
        "def text_to_speech(text):\n",
        "    # Convert text to speech and save as an MP3 file\n",
        "    if use_gtts:\n",
        "      if not isinstance(text, str):\n",
        "            text = str(text) # Making sure the text is a string not a list\n",
        "      tts = gTTS(text)\n",
        "      tts.save(\"response.mp3\")"
      ],
      "metadata": {
        "id": "hHZ3th8Rdxb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "yWL8u0g8Em1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import machine_learning\n",
        "from machine_learning import ml_agent"
      ],
      "metadata": {
        "id": "edZKk7Q2FzVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought(COT)"
      ],
      "metadata": {
        "id": "UHjBYt-f6S4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import chain_of_thought_reasoning\n",
        "from reason import memory_reasoning_thread # import memory reasoning thread96"
      ],
      "metadata": {
        "id": "5SLXIJHF6kTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI agent : the messages and prompts for memory agent tasks\n",
        "download(\"commons\",\"cot_messages_c6.py\") # downloaded messages and prompts"
      ],
      "metadata": {
        "id": "-1m3IiYhzhov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "id": "F3ufJsT54EHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z2abuU9OkVFL"
      },
      "outputs": [],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "id": "2eIyjdd25LjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "id": "0zPS_mgsdXKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "##  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO7AYljM0gq1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖAI Agent"
      ],
      "metadata": {
        "id": "-gGgBHAbZCGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic Synthetic Trajectory Simulation and Predictive System"
      ],
      "metadata": {
        "id": "5JiCaUiH-oRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Synthetic Trajectory Simulation"
      ],
      "metadata": {
        "id": "VesAR25Ywxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def create_grid_with_trajectory(grid_size=200, num_points=50, missing_count=5):\n",
        "    grid = np.zeros((grid_size, grid_size), dtype=int)\n",
        "    trajectory = []\n",
        "\n",
        "    x = random.randint(0, grid_size - 1)\n",
        "    y = random.randint(0, grid_size - 1)\n",
        "    day = random.randint(1, 365)\n",
        "    timeslot = random.randint(0, 47)\n",
        "\n",
        "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "    current_dir_index = random.randint(0, 3)\n",
        "\n",
        "    turn_weights = {-1: 0.15, 0: 0.70, 1: 0.15}\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        turn = random.choices(list(turn_weights.keys()), weights=list(turn_weights.values()))[0]\n",
        "        current_dir_index = (current_dir_index + turn) % len(directions)\n",
        "        dx, dy = directions[current_dir_index]\n",
        "        new_x = x + dx\n",
        "        new_y = y + dy\n",
        "\n",
        "        if not (0 <= new_x < grid_size and 0 <= new_y < grid_size):\n",
        "            valid_indices = [idx for idx, (dx_temp, dy_temp) in enumerate(directions) if 0 <= x + dx_temp < grid_size and 0 <= y + dy_temp < grid_size]\n",
        "            if valid_indices:\n",
        "                current_dir_index = random.choice(valid_indices)\n",
        "                dx, dy = directions[current_dir_index]\n",
        "                new_x = x + dx\n",
        "                new_y = y + dy\n",
        "            else:\n",
        "                new_x, new_y = x, y\n",
        "\n",
        "        x, y = new_x, new_y\n",
        "        trajectory.append((day, timeslot, x, y))\n",
        "        grid[x, y] = 1\n",
        "        timeslot = (timeslot + random.randint(1, 3)) % 48\n",
        "\n",
        "    missing_indices = random.sample(range(len(trajectory)), min(missing_count, len(trajectory)))\n",
        "    for idx in missing_indices:\n",
        "        d, t, _, _ = trajectory[idx]\n",
        "        trajectory[idx] = (d, t, 999, 999)\n",
        "\n",
        "    x_coords = [x if x != 999 else np.nan for _, _, x, y in trajectory]\n",
        "    y_coords = [y if y != 999 else np.nan for _, _, x, y in trajectory]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='blue', label=\"Agent Trajectory\")\n",
        "\n",
        "    valid_indices = [i for i, (xx, yy) in enumerate(zip(x_coords, y_coords)) if not (np.isnan(xx) or np.isnan(yy))]\n",
        "    if len(valid_indices) > 1:\n",
        "        valid_x = [x_coords[i] for i in valid_indices]\n",
        "        valid_y = [y_coords[i] for i in valid_indices]\n",
        "        dx = np.diff(valid_x)\n",
        "        dy = np.diff(valid_y)\n",
        "        plt.quiver(valid_x[:-1], valid_y[:-1], dx, dy, angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label=\"Direction\")\n",
        "\n",
        "    for (xx, yy) in zip(x_coords, y_coords):\n",
        "        if not np.isnan(xx) and not np.isnan(yy):\n",
        "            plt.plot(xx, yy, marker='D', markersize=10, color='green', label='Start')\n",
        "            break\n",
        "\n",
        "    for i, (d, t, x, y) in enumerate(trajectory):\n",
        "        if x == 999 and y == 999:\n",
        "            prev_valid = next_valid = None\n",
        "            for j in range(i - 1, -1, -1):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    prev_valid = (xp, yp)\n",
        "                    break\n",
        "            for j in range(i + 1, len(trajectory)):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    next_valid = (xp, yp)\n",
        "                    break\n",
        "            marker_x, marker_y = prev_valid if prev_valid else next_valid if next_valid else (None, None)\n",
        "            if marker_x is not None:\n",
        "                plt.plot(marker_x, marker_y, marker='x', markersize=10, color='magenta', label='Missing Data' if i == missing_indices[0] else \"\")\n",
        "\n",
        "    plt.title(\"Agent Trajectory with Direction Arrows and Missing Data\")\n",
        "    plt.xlabel(\"X coordinate\")\n",
        "    plt.ylabel(\"Y coordinate\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"mobility.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return grid, trajectory"
      ],
      "metadata": {
        "id": "ighe4XgT-OBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Predictive Engine"
      ],
      "metadata": {
        "id": "_IOEVVQF-ceC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b):\n",
        "    grid, trajectory = create_grid_with_trajectory(grid_size=200, num_points=50, missing_count=5)\n",
        "\n",
        "    trajectory_json = json.dumps({\"trajectory\": trajectory}, indent=2)\n",
        "    #print(\"Trajectory Data (JSON):\\n\", trajectory_json)\n",
        "    muser_message = f\"{muser_message1}\\n\\nHere is the trajectory data:\\n{trajectory_json}\"\n",
        "\n",
        "    reasoning_steps = reason.mobility_agent_reasoning_thread(\n",
        "        muser_message, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b\n",
        "    )\n",
        "\n",
        "    reasoning_steps.insert(0, (\"Generated Trajectory Data:\", trajectory))\n",
        "\n",
        "    return reasoning_steps"
      ],
      "metadata": {
        "id": "9mJhYF2I-eJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AI Functions"
      ],
      "metadata": {
        "id": "ta2BcIabK8zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Image\n",
        "import requests\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Global variable to ensure memory is always used\n",
        "memory_enabled = True  # Set to True to retain conversation memory\n",
        "\n",
        "# AI agent: Download messages and prompts\n",
        "download(\"commons\", \"cot_messages_c6.py\")  # Downloaded messages and prompts\n",
        "\n",
        "# Define Handler Functions\n",
        "def handle_pinecone_rag(user_message, **kwargs):\n",
        "    if \"Pinecone\" in user_message:\n",
        "      namespace = \"genaisys\"\n",
        "    if \"RAG\" in user_message:\n",
        "      namespace = \"data01\"\n",
        "\n",
        "    print(namespace)\n",
        "\n",
        "    query_text = user_message\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    print(\"Processed query results:\")\n",
        "    qtext, target_id = display_results(query_results)\n",
        "    print(qtext)\n",
        "\n",
        "    # Run task\n",
        "    sc_input = qtext + \" \" + user_message\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "       models=\"OpenAI\"\n",
        "\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      task_response = reason.make_openai_api_call(\n",
        "      sc_input, \"system\",\"You are an assistant who executes the tasks you are asked to do.\", \"user\")\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      inputs = tokenizer(sc_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"{namespace}:{models}: {task_response}\"\n",
        "\n",
        "def handle_reasoning_customer(user_message, **kwargs):\n",
        "    initial_query = user_message\n",
        "    download(\"Chapter05\", \"customer_activities.csv\")\n",
        "\n",
        "    reasoning_steps = reason.chain_of_thought_reasoning(initial_query)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_analysis(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "      models=\"OpenAI\"\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      reasoning_steps = reason.make_openai_reasoning_call(user_message, system_message_s1)\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      ds_input=system_message_s1+user_message\n",
        "      inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      reasoning_steps = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_generation(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1, generation, imcontent4, imcontent4b\n",
        "    reasoning_steps = reason.memory_reasoning_thread(user_message, system_message_s1, generation, imcontent4, imcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_mobility(user_message, **kwargs):\n",
        "    from cot_messages_c6 import msystem_message_s1, mgeneration, mimcontent4,muser_message1\n",
        "    mimcontent4b=mimcontent4\n",
        "    #call Generic Synthetic Trajectory Simulation and Predictive System\n",
        "    reasoning_steps = handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "\n",
        "def handle_image_creation(user_message, **kwargs):\n",
        "    prompt = user_message\n",
        "    image_url = reason.generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1)\n",
        "\n",
        "    # Save the image locally\n",
        "    save_path = \"c_image.png\"\n",
        "    image_data = requests.get(image_url).content\n",
        "    with open(save_path, \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "    return \"Image created\"\n",
        "\n",
        "'''\n",
        "def handle_no_memory(user_message, **kwargs):\n",
        "    task_response = reason.make_openai_api_call(\n",
        "        user_message, \"system\",\n",
        "        \"You are an assistant who executes the tasks you are asked to do.\", \"user\"\n",
        "    )\n",
        "    return task_response\n",
        "'''\n",
        "def handle_with_memory(messages, user_message, **kwargs):\n",
        "    global memory_enabled  # Ensure global memory setting is used\n",
        "\n",
        "    # If memory is disabled, respond with a message\n",
        "    if not memory_enabled:\n",
        "        return \"Memory is disabled.\"\n",
        "\n",
        "    # Extract all past messages (user + assistant) from the conversation history\n",
        "    conversation_history = [\n",
        "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
        "        for msg in messages if \"content\" in msg\n",
        "    ]\n",
        "\n",
        "    # Combine all conversation history\n",
        "    combined_history = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Append the latest user message to the history\n",
        "    full_context = f\"{combined_history}\\nUser: {user_message}\"\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "        task_response = reason.make_openai_api_call(\n",
        "            full_context, \"system\",\n",
        "            \"You are an assistant who executes the tasks you are asked to do.\", \"user\"\n",
        "        )\n",
        "\n",
        "    elif models == \"DeepSeek\":\n",
        "        # Tokenize the full conversation history for DeepSeek\n",
        "        sys_prompt = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        ds_input = f\"{sys_prompt}\\n{full_context}\"\n",
        "        inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "\n",
        "        # Decode the output\n",
        "        task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store bot response in memory\n",
        "    messages.append({\"role\": \"assistant\", \"content\": task_response})\n",
        "\n",
        "    return task_response"
      ],
      "metadata": {
        "id": "AV1z3qUAEnbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler registry"
      ],
      "metadata": {
        "id": "3-QtPpjcY2W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handler Registry\n",
        "handlers = [\n",
        "    # Pinecone / RAG handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Pinecone\" in user_message or \"RAG\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_pinecone_rag(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Reasoning handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: all(keyword in user_message for keyword in [\"Use reasoning\", \"customer\", \"activities\"]),\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_reasoning_customer(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Analysis handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Analysis\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_analysis(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Generation handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Generation\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_generation(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Mobility handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Mobility\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_mobility(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Create image handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Create\" in user_message and \"image\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_image_creation(user_message, models=models)\n",
        "    )\n",
        "]\n",
        "\n",
        "# Append the fallback memory handler for when instruct is \"None\"\n",
        "handlers.append(\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"None\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_with_memory(\n",
        "            msg, user_message,\n",
        "            files_status=kwargs.get('files_status'),\n",
        "            instruct=instruct,\n",
        "            mem=memory_enabled,  # ‚úÖ Replace user_memory with memory_enabled\n",
        "            models=models\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6R8eghE5Yvx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler selection mechanism"
      ],
      "metadata": {
        "id": "2-As05JvY8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gpt(messages, user_message, files_status, active_instruct, models):\n",
        "    global memory_enabled  # Ensure memory is used if set globally\n",
        "\n",
        "    try:\n",
        "        # Iterate over handlers and execute the first matching one\n",
        "        for condition, handler in handlers:\n",
        "            if condition(messages, active_instruct, memory_enabled, models, user_message):\n",
        "                return handler(messages, active_instruct, memory_enabled, models, user_message, files_status=files_status)\n",
        "\n",
        "        # If no handler matched, default to memory handling with full conversation history\n",
        "        return handle_with_memory(\n",
        "            messages,  # ‚úÖ Now passing full message history\n",
        "            user_message,\n",
        "            files_status=files_status,\n",
        "            instruct=active_instruct,\n",
        "            mem=memory_enabled,  # ‚úÖ Ensuring memory usage\n",
        "            models=models\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred in the handler selection mechanism: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "Q0Yxn5hhYx1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖGenAISys IPython interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing text"
      ],
      "metadata": {
        "id": "J-Yk7SIy60qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_json_as_markdown(data, level=0):\n",
        "    \"\"\"Format JSON-like data as Markdown with proper indentation.\"\"\"\n",
        "    html_output = \"\"\n",
        "    indent = \"  \" * level\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "            html_output += format_json_as_markdown(value, level + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            html_output += format_json_as_markdown(item, level)\n",
        "    else:\n",
        "        html_output += f\"{indent}{data}<br>\\n\"\n",
        "\n",
        "    return html_output or \"\"  # Ensure a string is always returned"
      ],
      "metadata": {
        "id": "4iqA8Y1fEEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "    \"\"\"Format the content of an entry for Markdown display.\"\"\"\n",
        "    if entry['role'] == 'user':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: blue;'>{active_user}:</span>** {formatted_content}\"\n",
        "    elif entry['role'] == 'assistant':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: green;'>Agent:</span>** {formatted_content}\"\n",
        "    else:\n",
        "        return entry['content']  # Fallback for unrecognized roles"
      ],
      "metadata": {
        "id": "S8b2--nsFQqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄRunning the interface"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "from ipywidgets import Dropdown, Textarea, Button, Checkbox, VBox, Layout, Output\n",
        "from PIL import Image as PILImage\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create an output widget for reasoning steps\n",
        "reasoning_output = Output(layout=Layout(border=\"1px solid black\", padding=\"10px\", margin=\"10px\", width=\"100%\"))\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        pfiles = 1 if files_checkbox.value else 0\n",
        "        active_instruct = instruct_selector.value\n",
        "        selected_model = model_selector.value\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message, pfiles, active_instruct, models=selected_model)\n",
        "\n",
        "        # Append bot response to active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # If TTS is enabled, convert response to speech\n",
        "        if tts_checkbox.value:\n",
        "          if isinstance(response, list):\n",
        "              response = \" \".join(response)  # Convert list to string if necessary\n",
        "          text_to_speech(response)\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for entry in user_histories[active_user]:\n",
        "        formatted_entry = format_entry(entry)\n",
        "        display(Markdown(formatted_entry))\n",
        "\n",
        "    #Audio display\n",
        "    if os.path.exists(\"/content/response.mp3\"):\n",
        "      display(Audio(\"/content/response.mp3\", autoplay=True))\n",
        "      !rm /content/response.mp3\n",
        "\n",
        "\n",
        "    # Check if Files are enabled\n",
        "    if files_checkbox.value == True:\n",
        "        # Display c_image.png if it exists\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            original_image = PILImage.open(\"c_image.png\")\n",
        "            new_size = (original_image.width // 2, original_image.height // 2)\n",
        "            resized_image = original_image.resize(new_size)\n",
        "            display(resized_image)\n",
        "\n",
        "        # Display mobility.png if it exists and the \"Mobility\" instruction is selected\n",
        "        if os.path.exists(\"mobility.png\") and instruct_selector.value == \"Mobility\":\n",
        "            original_image = PILImage.open(\"mobility.png\")\n",
        "            display(original_image)\n",
        "\n",
        "    # Display interactive widgets\n",
        "    if conversation_active:\n",
        "        display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Display reasoning_output persistently\n",
        "    display(reasoning_output)\n",
        "\n",
        "# Function to handle submission (button click or Enter key)\n",
        "def handle_submission():\n",
        "    user_message = input_box.value.strip()  # Get input text\n",
        "    if user_message:\n",
        "        input_box.value = \"\"  # Clear input box\n",
        "\n",
        "        # Show \"Processing request...\" immediately\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Processing request...\")\n",
        "\n",
        "        # Check if instruct_selector is \"Analysis\" or \"Generation\"\n",
        "        if instruct_selector.value in [\"Analysis\", \"Generation\",\"Mobility\"]:\n",
        "            with reasoning_output:\n",
        "                reasoning_output.clear_output(wait=True)\n",
        "                print(\"Thinking...\")\n",
        "\n",
        "        # Process user message\n",
        "        chat(user_message)\n",
        "\n",
        "        # Indicate that processing is finished\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Process completed.\")\n",
        "\n",
        "# Function to handle submit button click\n",
        "def handle_button_click(sender):\n",
        "    handle_submission()\n",
        "\n",
        "# Function to handle Enter key press in the Textarea\n",
        "def handle_enter_key(change):\n",
        "    if change['new'].endswith(\"\\n\"):  # Detect Enter key press\n",
        "        handle_submission()\n",
        "\n",
        "# Function to update active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create dropdown for user selection\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create multi-line input box\n",
        "input_box = Textarea(\n",
        "    placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\",\n",
        "    layout=Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "# Create submit button\n",
        "submit_button = Button(description=\"Send\", button_style='primary')\n",
        "submit_button.on_click(handle_button_click)\n",
        "\n",
        "# Attach event handler for Enter key (FIXED)\n",
        "input_box.observe(handle_enter_key, names=\"value\")\n",
        "\n",
        "# Create checkboxes for toggles\n",
        "tts_checkbox = Checkbox(value=False, description='Voice Output', layout=Layout(width='20%'))\n",
        "files_checkbox = Checkbox(value=False, description='Files', layout=Layout(width='20%'))\n",
        "agent_checkbox = Checkbox(value=True, description='Agent', layout=Layout(width='20%'))\n",
        "\n",
        "\n",
        "def on_files_checkbox_change(change):\n",
        "    # Only remove images if the checkbox changed from True to False.\n",
        "    if change['old'] == True and change['new'] == False:\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            os.remove(\"c_image.png\")\n",
        "        if os.path.exists(\"mobility.png\"):\n",
        "            os.remove(\"mobility.png\")\n",
        "\n",
        "# Attach the observer to files_checkbox\n",
        "files_checkbox.observe(on_files_checkbox_change, names='value')\n",
        "\n",
        "# Function to update instruct selector\n",
        "def on_instruct_change(change):\n",
        "    global active_instruct\n",
        "    active_instruct = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Dropdown for reasoning type\n",
        "instruct_selector = Dropdown(\n",
        "    options=[\"None\", \"Analysis\", \"Generation\",\"Mobility\"],\n",
        "    value=\"None\",\n",
        "    description='Reasoning:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "instruct_selector.observe(on_instruct_change, names='value')\n",
        "\n",
        "# Dropdown for model selection\n",
        "model_selector = Dropdown(\n",
        "    options=[\"OpenAI\", \"DeepSeek\"],\n",
        "    value=\"OpenAI\",\n",
        "    description=\"Model:\",\n",
        "    layout=Layout(width=\"50%\")\n",
        ")\n",
        "\n",
        "# Display interactive widgets\n",
        "display(\n",
        "    VBox(\n",
        "        [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "        layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display reasoning output\n",
        "with reasoning_output:\n",
        "    reasoning_output.clear_output(wait=True)\n",
        "    print(\"Reasoning activated\")"
      ],
      "metadata": {
        "id": "MmQR8KvbRfwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display the conversation history"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** *Make sure you have had a conversation by running the interface in the preceding cell. Then make sure you stopped the conversion with 'quit' or 'exit\" so that the conversation is saved. If not, the conversation will not be displayed and summarized in the following cells.*\n"
      ],
      "metadata": {
        "id": "L7UFAv8SoSGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "display_conversation_history=True\n",
        "summary=True\n",
        "\n",
        "if display_conversation_history == True or summary==True:\n",
        "    # File path\n",
        "    file_path = 'conversation_history.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"The file '{file_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "        display_conversation_history=False\n",
        "        summary=False\n",
        "        print(\"Conversation history not processed\")"
      ],
      "metadata": {
        "id": "y8W1IpYdKyds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display option\n",
        "if display_conversation_history==True:\n",
        "  # File path\n",
        "  file_path = 'conversation_history.json'\n",
        "\n",
        "  # Open the file and read its content into the 'dialog' variable\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      dialog = json.load(file)  # Parse JSON content\n",
        "\n",
        "  # Function to format JSON content as markdown\n",
        "  def format_json_as_markdown(data, level=0):\n",
        "      html_output = \"\"\n",
        "      indent = \"  \" * level\n",
        "      if isinstance(data, dict):\n",
        "          for key, value in data.items():\n",
        "              html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "              html_output += format_json_as_markdown(value, level + 1)\n",
        "      elif isinstance(data, list):\n",
        "          for item in data:\n",
        "              html_output += format_json_as_markdown(item, level)\n",
        "      else:\n",
        "          html_output += f\"{indent}{data}<br>\\n\"\n",
        "      return html_output\n",
        "\n",
        "  # Format the JSON into markdown\n",
        "  formatted_markdown = format_json_as_markdown(dialog)\n",
        "\n",
        "  # Display formatted JSON as Markdown\n",
        "  display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "id": "OcPwASR3cuSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize the conversation history"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def summarize_conversation(file_path):\n",
        "    \"\"\"\n",
        "    Reads a conversation history JSON file, formats it, and generates a detailed\n",
        "    summary with a list of actions from the JSON text. The summary is displayed in Markdown.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file containing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        None: The summary is displayed as Markdown output.\n",
        "    \"\"\"\n",
        "    # Step 1: Read the conversation history from the JSON file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dialog = file.read()\n",
        "    conversation_history_json = json.loads(dialog)\n",
        "\n",
        "    # Step 2: Construct dialog string from the JSON conversation history\n",
        "    def construct_dialog(conversation_history_json):\n",
        "        dialog = \"\"\n",
        "        for user, messages in conversation_history_json.items():\n",
        "            dialog += f\"\\n{user}:\\n\"\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "                dialog += f\"- {role}: {content}\\n\"\n",
        "        return dialog\n",
        "\n",
        "    formatted_dialog = construct_dialog(conversation_history_json)\n",
        "\n",
        "    # Step 3: Prepare the task for the summary\n",
        "    mrole = \"system\"\n",
        "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "    user_role = \"user\"\n",
        "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "    # Step 4: Call the `make_openai_api_call` function\n",
        "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "    # Step 5: Display the task response as Markdown\n",
        "    display(Markdown(task_response))\n"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if summary==True:\n",
        "    # File path to the JSON file\n",
        "    file_path = '/content/conversation_history.json'\n",
        "\n",
        "    # Check if the file exists before calling the function\n",
        "    if os.path.exists(file_path):\n",
        "        summarize_conversation(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
      ],
      "metadata": {
        "id": "S6iA-jOUj8s-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}